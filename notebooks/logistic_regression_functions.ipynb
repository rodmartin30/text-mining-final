{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from ipynb.fs.full.utils_functions import load_corpus\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING CLASSIFIER with own Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_predictions(model,X_test,k):\n",
    "    \n",
    "    # get probabilities instead of predicted labels, since we want to collect top 3\n",
    "    probs = model.predict_proba(X_test)\n",
    "\n",
    "    # GET TOP K PREDICTIONS BY PROB - note these are just index\n",
    "    best_n = np.argsort(probs, axis=1)[:,-k:]\n",
    "    \n",
    "    # GET CATEGORY OF PREDICTIONS\n",
    "    preds=[[model.classes_[predicted_cat] for predicted_cat in prediction] for prediction in best_n]\n",
    "    \n",
    "    preds=[ item[::-1] for item in preds]\n",
    "    \n",
    "    return preds\n",
    "\n",
    "def extract_features(df,field,training_data,testing_data,type=\"binary\"):\n",
    "    \"\"\"Extract features using different methods\"\"\"\n",
    "    \n",
    "    logging.info(\"Extracting features and creating vocabulary...\")\n",
    "    \n",
    "    if \"binary\" in type:\n",
    "        \n",
    "        # BINARY FEATURE REPRESENTATION\n",
    "        cv= CountVectorizer(binary=True, max_df=0.95)\n",
    "        cv.fit_transform(training_data[field].values)\n",
    "        \n",
    "        train_feature_set=cv.transform(training_data[field].values)\n",
    "        test_feature_set=cv.transform(testing_data[field].values)\n",
    "        \n",
    "        return train_feature_set,test_feature_set,cv\n",
    "  \n",
    "    elif \"counts\" in type:\n",
    "        \n",
    "        # COUNT BASED FEATURE REPRESENTATION\n",
    "        cv= CountVectorizer(binary=False, max_df=0.95, ngram_range = (3,3))\n",
    "        cv.fit_transform(training_data[field].values)\n",
    "        \n",
    "        train_feature_set=cv.transform(training_data[field].values)\n",
    "        test_feature_set=cv.transform(testing_data[field].values)\n",
    "        \n",
    "        return train_feature_set,test_feature_set,cv\n",
    "    \n",
    "    else:    \n",
    "        \n",
    "        # TF-IDF BASED FEATURE REPRESENTATION\n",
    "        tfidf_vectorizer=TfidfVectorizer(use_idf=True, max_df=0.95, ngram_range = (3,3))\n",
    "        tfidf_vectorizer.fit_transform(training_data[field].values)\n",
    "        \n",
    "        train_feature_set=tfidf_vectorizer.transform(training_data[field].values)\n",
    "        test_feature_set=tfidf_vectorizer.transform(testing_data[field].values)\n",
    "        \n",
    "        return train_feature_set,test_feature_set,tfidf_vectorizer\n",
    "\n",
    "    \n",
    "    \n",
    "def train_model_with_transformer(df,field=\"text\",feature_rep=\"binary\",top_k=3):\n",
    "    \n",
    "    logging.info(\"Starting model training...\")\n",
    "    \n",
    "    # GET A TRAIN TEST SPLIT (set seed for consistent results)\n",
    "    training_data, testing_data = train_test_split(df,random_state = 2000)\n",
    "\n",
    "    # GET LABELS\n",
    "    Y_train=training_data['label'].values\n",
    "    Y_test=testing_data['label'].values\n",
    "\n",
    "    # GET FEATURES\n",
    "    X_train,X_test,feature_transformer=extract_features(df,field,training_data,testing_data,type=feature_rep)\n",
    "    \n",
    "    # INIT LOGISTIC REGRESSION CLASSIFIER\n",
    "    logging.info(\"Training a Logistic Regression Model...\")\n",
    "    scikit_log_reg = LogisticRegression(verbose=1, solver='liblinear',random_state=0, C=5, penalty='l2',max_iter=1000)\n",
    "    model=scikit_log_reg.fit(X_train,Y_train)\n",
    "\n",
    "    # GET TOP K PREDICTIONS\n",
    "    preds=get_top_k_predictions(model,X_test,top_k)\n",
    "    \n",
    "    # GET PREDICTED VALUES AND GROUND TRUTH INTO A LIST OF LISTS - for ease of evaluation\n",
    "    eval_items=collect_preds(Y_test,preds)\n",
    "    \n",
    "    # GET EVALUATION NUMBERS ON TEST SET -- HOW DID WE DO?\n",
    "    logging.info(\"Starting evaluation...\")\n",
    "    accuracy=compute_accuracy(eval_items)\n",
    "    mrr_at_k=compute_mrr_at_k(eval_items)\n",
    "\n",
    "    logging.info(\"Done training and evaluation.\")\n",
    "    \n",
    "    return model,feature_transformer,accuracy,mrr_at_k\n",
    "\n",
    "def train_model(df,field=\"text\",top_k=3):\n",
    "    \n",
    "    logging.info(\"Starting model training...\")\n",
    "    \n",
    "    # GET A TRAIN TEST SPLIT (set seed for consistent results)\n",
    "    training_data, testing_data = train_test_split(df,random_state = 2000,)\n",
    "\n",
    "    # GET LABELS\n",
    "    Y_train=training_data['label'].values.tolist()\n",
    "    Y_test=testing_data['label'].values.tolist()\n",
    "     \n",
    "    # GET FEATURES\n",
    "    X_train=training_data['text'].values.tolist()\n",
    "    X_test=testing_data['text'].values.tolist()\n",
    "    \n",
    "    \n",
    "    # INIT LOGISTIC REGRESSION CLASSIFIER\n",
    "    logging.info(\"Training a Logistic Regression Model...\")\n",
    "    scikit_log_reg = LogisticRegression(verbose=1, solver='liblinear', multi_class='auto', random_state=0, C=5, penalty='l2',max_iter=1000)\n",
    "    model=scikit_log_reg.fit(X_train,Y_train)\n",
    "\n",
    "    # GET TOP K PREDICTIONS\n",
    "    preds=get_top_k_predictions(model,X_test,top_k)\n",
    "    \n",
    "    # GET PREDICTED VALUES AND GROUND TRUTH INTO A LIST OF LISTS - for ease of evaluation\n",
    "    eval_items=collect_preds(Y_test,preds)\n",
    "    \n",
    "    # GET EVALUATION NUMBERS ON TEST SET -- HOW DID WE DO?\n",
    "    logging.info(\"Starting evaluation...\")\n",
    "    accuracy=compute_accuracy(eval_items)\n",
    "    mrr_at_k=compute_mrr_at_k(eval_items)\n",
    "\n",
    "    logging.info(\"Done training and evaluation.\")\n",
    "    \n",
    "    return model,accuracy,mrr_at_k\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reciprocal_rank(true_labels: list, machine_preds: list):\n",
    "    \"\"\"Compute the reciprocal rank at cutoff k\"\"\"\n",
    "    \n",
    "    # add index to list only if machine predicted label exists in true labels\n",
    "    tp_pos_list = [(idx + 1) for idx, r in enumerate(machine_preds) if r in true_labels]\n",
    "\n",
    "    rr = 0\n",
    "    if len(tp_pos_list) > 0:\n",
    "        # for RR we need position of first correct item\n",
    "        first_pos_list = tp_pos_list[0]\n",
    "        \n",
    "        # rr = 1/rank\n",
    "        rr = 1 / float(first_pos_list)\n",
    "\n",
    "    return rr\n",
    "\n",
    "def compute_mrr_at_k(items:list):\n",
    "    \"\"\"Compute the MRR (average RR) at cutoff k\"\"\"\n",
    "    rr_total = 0\n",
    "    \n",
    "    for item in items:   \n",
    "        rr_at_k = _reciprocal_rank(item[0],item[1])\n",
    "        rr_total = rr_total + rr_at_k\n",
    "        mrr = rr_total / 1/float(len(items))\n",
    "\n",
    "    return mrr\n",
    "\n",
    "def collect_preds(Y_test,Y_preds):\n",
    "    \"\"\"Collect all predictions and ground truth\"\"\"\n",
    "    \n",
    "    pred_gold_list=[[[Y_test[idx]],pred] for idx,pred in enumerate(Y_preds)]\n",
    "    return pred_gold_list\n",
    "             \n",
    "def compute_accuracy(eval_items:list):\n",
    "    correct=0\n",
    "    total=0\n",
    "    \n",
    "    for item in eval_items:\n",
    "        true_pred=item[0]\n",
    "        machine_pred=set(item[1])\n",
    "        \n",
    "        for cat in true_pred:\n",
    "            if cat in machine_pred:\n",
    "                correct+=1\n",
    "                break\n",
    "    \n",
    "    \n",
    "    accuracy=correct/float(len(eval_items))\n",
    "    return accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
